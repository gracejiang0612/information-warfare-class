{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd51629-1e1b-437a-96e1-d46ba9e0e27a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 145\u001b[0m\n\u001b[1;32m    142\u001b[0m     usernames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrape_x_usernames(search_term)  \u001b[38;5;66;03m# Just usernames\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/asyncio/runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import time\n",
    "import json\n",
    "\n",
    "async def scrape_x_usernames(search_term):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)  # Set to True for production\n",
    "        context = await browser.new_context(\n",
    "            viewport={'width': 1280, 'height': 800},\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "        )\n",
    "        \n",
    "        page = await context.new_page()\n",
    "        \n",
    "        try:\n",
    "            # Navigate to the search page\n",
    "            url = f\"https://x.com/search?q={search_term}&src=typed_query&f=user\"\n",
    "            await page.goto(url)\n",
    "            \n",
    "            # Wait for the page to load\n",
    "            await page.wait_for_selector(\"section[role='region']\", timeout=10000)\n",
    "            \n",
    "            # Scroll down to load more results (adjust as needed)\n",
    "            for _ in range(3):\n",
    "                await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                await asyncio.sleep(2)  # Use asyncio.sleep instead of time.sleep\n",
    "            \n",
    "            # Extract usernames using the pattern visible in your screenshot\n",
    "            username_elements = await page.query_selector_all(\"div[dir='ltr'] span:has-text('@')\")\n",
    "            \n",
    "            usernames = []\n",
    "            for elem in username_elements:\n",
    "                username = await elem.inner_text()\n",
    "                username = username.strip().replace('@', '')\n",
    "                usernames.append(username)\n",
    "                \n",
    "            print(f\"Found {len(usernames)} usernames:\")\n",
    "            for username in usernames:\n",
    "                print(username)\n",
    "                \n",
    "            # Save to a file if needed\n",
    "            with open(f\"x_usernames_{search_term}.json\", \"w\") as f:\n",
    "                json.dump(usernames, f, indent=2)\n",
    "                \n",
    "            return usernames\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "# Full version with more details\n",
    "async def scrape_x_accounts(search_term):\n",
    "    \"\"\"\n",
    "    Scrape X accounts based on a search term using Playwright's Async API\n",
    "    \n",
    "    Args:\n",
    "        search_term (str): The search term to find accounts\n",
    "        \n",
    "    Returns:\n",
    "        list: List of account information\n",
    "    \"\"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)  # Set to True for production\n",
    "        context = await browser.new_context(\n",
    "            viewport={'width': 1280, 'height': 800},\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "        )\n",
    "        \n",
    "        page = await context.new_page()\n",
    "        \n",
    "        try:\n",
    "            # Navigate to the search page\n",
    "            url = f\"https://x.com/search?q={search_term}&src=typed_query&f=user\"\n",
    "            await page.goto(url)\n",
    "            \n",
    "            # Wait for the page to load\n",
    "            await page.wait_for_selector(\"section[role='region']\", timeout=10000)\n",
    "            \n",
    "            # Scroll down to load more results (adjust as needed)\n",
    "            for _ in range(3):\n",
    "                await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                await asyncio.sleep(2)\n",
    "            \n",
    "            # Extract account information\n",
    "            accounts = []\n",
    "            \n",
    "            # Using appropriate selectors based on the HTML structure from your screenshot\n",
    "            account_elements = await page.query_selector_all(\"div[data-testid='cellInnerDiv']\")\n",
    "            \n",
    "            for element in account_elements:\n",
    "                try:\n",
    "                    # Get username\n",
    "                    username_element = await element.query_selector(\"div[dir='ltr'] span:has-text('@')\")\n",
    "                    if not username_element:\n",
    "                        continue\n",
    "                        \n",
    "                    username_text = await username_element.inner_text()\n",
    "                    username = username_text.strip().replace('@', '')\n",
    "                    \n",
    "                    # Get display name\n",
    "                    name_element = await element.query_selector(\"div[data-testid='User-Name'] span span\")\n",
    "                    display_name = await name_element.inner_text() if name_element else \"Unknown\"\n",
    "                    \n",
    "                    # Get bio if available\n",
    "                    bio_element = await element.query_selector(\"div[data-testid='UserCell-byline']\")\n",
    "                    bio = await bio_element.inner_text() if bio_element else \"\"\n",
    "                    \n",
    "                    accounts.append({\n",
    "                        \"username\": username,\n",
    "                        \"display_name\": display_name.strip(),\n",
    "                        \"bio\": bio.strip(),\n",
    "                        \"profile_url\": f\"https://x.com/{username}\"\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting account info: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save to JSON file\n",
    "            with open(f\"x_accounts_{search_term}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(accounts, f, indent=2)\n",
    "                \n",
    "            print(f\"Found {len(accounts)} accounts for search term '{search_term}'\")\n",
    "            return accounts\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "# Example usage - this is how you run the async functions\n",
    "async def main():\n",
    "    search_term = \"Roofies\"  # Based on your screenshot\n",
    "    # Choose which function to use\n",
    "    # accounts = await scrape_x_accounts(search_term)  # Full account details\n",
    "    usernames = await scrape_x_usernames(search_term)  # Just usernames\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097ffb6d-8150-4925-86ef-2f4c64f7e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Page.wait_for_selector: Timeout 10000ms exceeded.\n",
      "Call log:\n",
      "  - waiting for locator(\"section[role='region']\") to be visible\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import json\n",
    "\n",
    "async def scrape_x_usernames(search_term):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)  # Set to True for production\n",
    "        context = await browser.new_context(\n",
    "            viewport={'width': 1280, 'height': 800},\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "        )\n",
    "        \n",
    "        page = await context.new_page()\n",
    "        \n",
    "        try:\n",
    "            # Navigate to the search page\n",
    "            url = f\"https://x.com/search?q={search_term}&src=typed_query&f=user\"\n",
    "            await page.goto(url)\n",
    "            \n",
    "            # Wait for the page to load\n",
    "            await page.wait_for_selector(\"section[role='region']\", timeout=10000)\n",
    "            \n",
    "            # Scroll down to load more results (adjust as needed)\n",
    "            for _ in range(3):\n",
    "                await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                await asyncio.sleep(2)  # Use asyncio.sleep instead of time.sleep\n",
    "            \n",
    "            # Extract usernames using the pattern visible in your screenshot\n",
    "            username_elements = await page.query_selector_all(\"div[dir='ltr'] span:has-text('@')\")\n",
    "            \n",
    "            usernames = []\n",
    "            for elem in username_elements:\n",
    "                username = await elem.inner_text()\n",
    "                username = username.strip().replace('@', '')\n",
    "                usernames.append(username)\n",
    "                \n",
    "            print(f\"Found {len(usernames)} usernames:\")\n",
    "            for username in usernames:\n",
    "                print(username)\n",
    "                \n",
    "            # Save to a file if needed\n",
    "            with open(f\"x_usernames_{search_term}.json\", \"w\") as f:\n",
    "                json.dump(usernames, f, indent=2)\n",
    "                \n",
    "            return usernames\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "# Here's the modified part that works with existing event loops:\n",
    "def run_async_scraper(search_term):\n",
    "    \"\"\"Run the async scraper in any environment\"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # We're in a Jupyter notebook, IPython, or similar environment\n",
    "        # Use asyncio.ensure_future or create_task\n",
    "        return asyncio.create_task(scrape_x_usernames(search_term))\n",
    "    else:\n",
    "        # We're in a regular Python script \n",
    "        return loop.run_until_complete(scrape_x_usernames(search_term))\n",
    "\n",
    "# Example usage that works in both regular Python scripts and interactive environments\n",
    "if __name__ == \"__main__\":\n",
    "    # This will work in a regular Python script\n",
    "    search_term = \"Roofies\"  # Based on your screenshot\n",
    "    usernames = run_async_scraper(search_term)\n",
    "else:\n",
    "    # For interactive environments, you can call this directly\n",
    "    # search_term = \"Roofies\"\n",
    "    # task = run_async_scraper(search_term)\n",
    "    # If you're in a Jupyter notebook or similar, you would do:\n",
    "    # usernames = await task  # You need to await the task\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4876c638-471b-4987-a0b9-23a23df6bab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 126\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    125\u001b[0m     search_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoofies\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Based on your screenshot\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     usernames \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_x_usernames_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_term\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mscrape_x_usernames_sync\u001b[0;34m(search_term)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_x_usernames_sync\u001b[39m(search_term):\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# See the browser in action\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mviewport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/playwright/sync_api/_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m             )\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "import json\n",
    "import time\n",
    "\n",
    "def scrape_x_usernames_sync(search_term):\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=False)  # See the browser in action\n",
    "        context = browser.new_context(\n",
    "            viewport={'width': 1280, 'height': 800},\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "        )\n",
    "        \n",
    "        page = context.new_page()\n",
    "        \n",
    "        try:\n",
    "            # Navigate to the search page\n",
    "            url = f\"https://x.com/search?q={search_term}&src=typed_query&f=user\"\n",
    "            page.goto(url)\n",
    "            \n",
    "            # Give the page some time to load initially\n",
    "            time.sleep(5)\n",
    "            \n",
    "            print(\"Waiting for content to load...\")\n",
    "            \n",
    "            # Try different selectors that appear in the search results page\n",
    "            selectors_to_try = [\n",
    "                \"div[data-testid='cellInnerDiv']\",  # Common cell container\n",
    "                \"a[role='link'][tabindex='0']\",     # Links in the results\n",
    "                \"span:has-text('@')\",               # Username spans\n",
    "                \"div[data-testid='User-Name']\",     # User name container\n",
    "                \"div[dir='ltr']\",                   # Text direction containers (common on X)\n",
    "                \"main[role='main']\"                 # Main content area\n",
    "            ]\n",
    "            \n",
    "            found_selector = None\n",
    "            for selector in selectors_to_try:\n",
    "                if page.locator(selector).count() > 0:\n",
    "                    print(f\"Found elements with selector: {selector}\")\n",
    "                    found_selector = selector\n",
    "                    break\n",
    "            \n",
    "            if not found_selector:\n",
    "                print(\"Could not find any recognizable elements on the page.\")\n",
    "                # Try to save the page content for debugging\n",
    "                page_content = page.content()\n",
    "                with open(f\"debug_page_{search_term}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(page_content)\n",
    "                print(f\"Saved page content to debug_page_{search_term}.html for inspection\")\n",
    "                return []\n",
    "            \n",
    "            # Scroll down to load more results (adjust as needed)\n",
    "            print(\"Scrolling to load more results...\")\n",
    "            for i in range(3):\n",
    "                page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                time.sleep(2)\n",
    "                print(f\"Scroll {i+1}/3 completed\")\n",
    "            \n",
    "            # Look for username elements more carefully\n",
    "            print(\"Extracting usernames...\")\n",
    "            \n",
    "            # Try multiple selector patterns for usernames based on your screenshot\n",
    "            username_patterns = [\n",
    "                \"div[dir='ltr'] span:has-text('@')\",\n",
    "                \"a[role='link'] span:has-text('@')\",\n",
    "                \"span:has-text('@')\",\n",
    "                \"div[data-testid='User-Name'] div[dir='ltr']\"\n",
    "            ]\n",
    "            \n",
    "            usernames = []\n",
    "            for pattern in username_patterns:\n",
    "                elements = page.query_selector_all(pattern)\n",
    "                if elements:\n",
    "                    print(f\"Found {len(elements)} elements with pattern: {pattern}\")\n",
    "                    for elem in elements:\n",
    "                        text = elem.inner_text()\n",
    "                        if '@' in text:\n",
    "                            username = text.strip().replace('@', '')\n",
    "                            if username and username not in usernames:\n",
    "                                usernames.append(username)\n",
    "            \n",
    "            # If we still don't have usernames, try a more general approach\n",
    "            if not usernames:\n",
    "                print(\"Trying alternative approach to find usernames...\")\n",
    "                # Get all text elements on the page\n",
    "                all_text_elements = page.query_selector_all(\"span\")\n",
    "                for elem in all_text_elements:\n",
    "                    text = elem.inner_text()\n",
    "                    if '@' in text and not text.startswith('@'):\n",
    "                        potential_username = text.strip()\n",
    "                        # Look for typical username pattern (@ followed by letters/numbers)\n",
    "                        if '@' in potential_username:\n",
    "                            username = potential_username.split('@')[1].split()[0]\n",
    "                            if username and username not in usernames:\n",
    "                                usernames.append(username)\n",
    "            \n",
    "            print(f\"Found {len(usernames)} usernames:\")\n",
    "            for username in usernames:\n",
    "                print(username)\n",
    "                \n",
    "            # Take a screenshot for debugging purposes\n",
    "            page.screenshot(path=f\"search_results_{search_term}.png\")\n",
    "            print(f\"Saved screenshot to search_results_{search_term}.png\")\n",
    "            \n",
    "            # Save to a file\n",
    "            with open(f\"x_usernames_{search_term}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(usernames, f, indent=2)\n",
    "                \n",
    "            return usernames\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            # Try to capture a screenshot of the error state\n",
    "            try:\n",
    "                page.screenshot(path=f\"error_state_{search_term}.png\")\n",
    "                print(f\"Saved error state screenshot to error_state_{search_term}.png\")\n",
    "            except:\n",
    "                pass\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            browser.close()\n",
    "\n",
    "# Example usage - simple synchronous call\n",
    "if __name__ == \"__main__\":\n",
    "    search_term = \"Roofies\"  # Based on your screenshot\n",
    "    usernames = scrape_x_usernames_sync(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833b7f5-1495-4cd7-bca9-2840d9ee3948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
